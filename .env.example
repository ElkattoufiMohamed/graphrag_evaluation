# ====== Core experiment switches ======
# LLM provider used by BOTH baseline and GraphRAG runs: qwen | gemini | ollama
EVAL_LLM_PROVIDER=qwen
# Example models:
# - qwen: qwen-plus
# - gemini: gemini-1.5-pro
# - ollama: qcwind/qwen2.5-7B-instruct-Q4_K_M
EVAL_LLM_MODEL=qwen-plus

# Embedding backend used by BOTH groups: st | openai
# st     -> BAAI/bge-m3 (local sentence-transformers)
# openai -> text-embedding-3-small
EVAL_EMBEDDING_BACKEND=st

# Device for local embedding model (GraphRAG side)
EMBED_DEVICE=cpu

# ====== API keys ======
# Required when EVAL_LLM_PROVIDER=qwen
DASHSCOPE_API_KEY=your_dashscope_key_here
# Region endpoint must match the API key region.
# International (Singapore): https://dashscope-intl.aliyuncs.com/api/v1
# China (Beijing):         https://dashscope.aliyuncs.com/api/v1
# US (Virginia):           https://dashscope-us.aliyuncs.com/api/v1
DASHSCOPE_BASE_HTTP_API_URL=https://dashscope-intl.aliyuncs.com/api/v1

# Required when EVAL_LLM_PROVIDER=gemini
GEMINI_API_KEY=your_gemini_key_here

# Required when EVAL_LLM_PROVIDER=ollama (local server)
OLLAMA_BASE_URL=http://127.0.0.1:11434
OLLAMA_TIMEOUT_SECONDS=1800
OLLAMA_MAX_RETRIES=3
# Keep generation bounded for GraphRAG extraction speed/stability on local hardware.
OLLAMA_NUM_PREDICT=512
OLLAMA_NUM_CTX=8192

# GraphRAG LLM concurrency (set to 1 for local Ollama stability)
GRAPHRAG_LLM_CONCURRENCY=1
# Try to auto-repair malformed JSON outputs from local models during GraphRAG indexing
GRAPHRAG_JSON_REPAIR=1

# Required when EVAL_EMBEDDING_BACKEND=openai
OPENAI_API_KEY=your_openai_key_here
